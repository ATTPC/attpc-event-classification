{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading simulated data\n",
    "p_sim = sp.sparse.load_npz('../../data/tilt/20x20x20/pDisc_noise_40000_20x20x20_tilt.npz')\n",
    "C_sim = sp.sparse.load_npz('../../data/tilt/20x20x20/CDisc_noise_40000_20x20x20_tilt.npz')\n",
    "junk_sim = sp.sparse.load_npz('../../data/tilt/20x20x20/noiseDisc_40000_20x20x20.npz')\n",
    "\n",
    "#loading real data\n",
    "p_0130 = sp.sparse.load_npz('../../data/real/20x20x20/run_0130_pDisc.npz')\n",
    "C_0130 = sp.sparse.load_npz('../../data/real/20x20x20/run_0130_CDisc.npz')\n",
    "junk_0130 = sp.sparse.load_npz('../../data/real/20x20x20/run_0130_junkDisc.npz')\n",
    "p_0210 = sp.sparse.load_npz('../../data/real/20x20x20/run_0210_pDisc.npz')\n",
    "C_0210 = sp.sparse.load_npz('../../data/real/20x20x20/run_0210_CDisc.npz')\n",
    "junk_0210 = sp.sparse.load_npz('../../data/real/20x20x20/run_0210_junkDisc.npz')\n",
    "\n",
    "p_real = sp.sparse.vstack([p_0130, p_0210], format='csr')\n",
    "C_real = sp.sparse.vstack([C_0130, C_0210], format='csr')\n",
    "junk_real = sp.sparse.vstack([junk_0130, junk_0210], format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating labels\n",
    "p_sim_labels = np.zeros((p_sim.shape[0],))\n",
    "C_sim_labels = np.ones((C_sim.shape[0],))\n",
    "junk_sim_labels = np.ones((junk_sim.shape[0],))\n",
    "\n",
    "p_real_labels = np.zeros((p_real.shape[0],))\n",
    "C_real_labels = np.ones((C_real.shape[0],))\n",
    "junk_real_labels = np.ones((junk_real.shape[0],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Totaling Charges by Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_sim_charge = []\n",
    "for i in range(p_sim.shape[0]):\n",
    "    p_sim_charge.append(p_sim[i].sum())\n",
    "    \n",
    "C_sim_charge = []\n",
    "for i in range(C_sim.shape[0]):\n",
    "    C_sim_charge.append(C_sim[i].sum())\n",
    "    \n",
    "junk_sim_charge = []\n",
    "for i in range(junk_sim.shape[0]):\n",
    "    junk_sim_charge.append(junk_sim[i].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_real_charge = []\n",
    "for i in range(p_real.shape[0]):\n",
    "    p_real_charge.append(p_real[i].sum())\n",
    "    \n",
    "C_real_charge = []\n",
    "for i in range(C_real.shape[0]):\n",
    "    C_real_charge.append(C_real[i].sum())\n",
    "    \n",
    "junk_real_charge = []\n",
    "for i in range(junk_real.shape[0]):\n",
    "    junk_real_charge.append(junk_real[i].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing LR with ONE feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting proton vs. Carbon (w/ Simulated Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80002, 1)\n",
      "(1003, 1)\n"
     ]
    }
   ],
   "source": [
    "pC_sim = np.reshape(p_sim_charge + C_sim_charge, (-1,1))\n",
    "pC_sim_labels = np.hstack((p_sim_labels, C_sim_labels))\n",
    "\n",
    "pC_real = np.reshape(p_real_charge + C_real_charge, (-1,1))\n",
    "pC_real_labels = np.hstack((p_real_labels, C_real_labels))\n",
    "\n",
    "print(pC_sim.shape)\n",
    "print(pC_real.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pC_sim_train, pC_sim_test, pC_sim_labels_train, pC_sim_labels_test = train_test_split(pC_sim, pC_sim_labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = 0.1\n",
    "lr_pC = LogisticRegression(C=reg)\n",
    "lr_pC.fit(pC_sim_train, pC_sim_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6246687665616719\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      0.66      0.64     10007\n",
      "        1.0       0.63      0.59      0.61      9994\n",
      "\n",
      "avg / total       0.63      0.62      0.62     20001\n",
      "\n",
      "[[6623 3384]\n",
      " [4123 5871]]\n"
     ]
    }
   ],
   "source": [
    "#train on simulated + test on simulated \n",
    "pC_sim_pred = lr_pC.predict(pC_sim_test)\n",
    "\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(pC_sim_labels_test, pC_sim_pred)))\n",
    "print(metrics.classification_report(pC_sim_labels_test, pC_sim_pred))\n",
    "print(metrics.confusion_matrix(pC_sim_labels_test, pC_sim_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8404785643070788\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.81      0.98      0.89       663\n",
      "        1.0       0.95      0.56      0.70       340\n",
      "\n",
      "avg / total       0.86      0.84      0.83      1003\n",
      "\n",
      "[[652  11]\n",
      " [149 191]]\n"
     ]
    }
   ],
   "source": [
    "#train on simulated + test on real\n",
    "pC_real_pred = lr_pC.predict(pC_real)\n",
    "\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(pC_real_labels, pC_real_pred)))\n",
    "print(metrics.classification_report(pC_real_labels, pC_real_pred))\n",
    "print(metrics.confusion_matrix(pC_real_labels, pC_real_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting proton vs. junk (w/ Simulated Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# proton 0s\n",
    "# junk 1s\n",
    "pjunk_sim = np.reshape(p_sim_charge + junk_sim_charge, (-1,1))\n",
    "pjunk_sim_labels = np.hstack((p_sim_labels, junk_sim_labels))\n",
    "\n",
    "pjunk_real = np.reshape(p_real_charge + junk_real_charge, (-1,1))\n",
    "pjunk_real_labels = np.hstack((p_real_labels, junk_real_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pjunk_sim_train, pjunk_sim_test, pjunk_sim_labels_train, pjunk_sim_labels_test = train_test_split(pjunk_sim, pjunk_sim_labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = 0.001\n",
    "lr_pjunk = LogisticRegression(C=reg)\n",
    "lr_pjunk.fit(pjunk_sim_train, pjunk_sim_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.544072796360182\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.58      0.56     10002\n",
      "        1.0       0.55      0.51      0.53      9999\n",
      "\n",
      "avg / total       0.54      0.54      0.54     20001\n",
      "\n",
      "[[5810 4192]\n",
      " [4927 5072]]\n"
     ]
    }
   ],
   "source": [
    "#train on simulated + test on simulated \n",
    "pjunk_sim_pred = lr_pjunk.predict(pjunk_sim_test)\n",
    "\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(pjunk_sim_labels_test, pjunk_sim_pred)))\n",
    "print(metrics.classification_report(pjunk_sim_labels_test, pjunk_sim_pred))\n",
    "print(metrics.confusion_matrix(pjunk_sim_labels_test, pjunk_sim_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6287782034908471\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.17      0.08      0.11       663\n",
      "        1.0       0.70      0.84      0.77      1686\n",
      "\n",
      "avg / total       0.55      0.63      0.58      2349\n",
      "\n",
      "[[  54  609]\n",
      " [ 263 1423]]\n"
     ]
    }
   ],
   "source": [
    "#train on simulated + test on real \n",
    "pjunk_real_pred = lr_pjunk.predict(pjunk_real)\n",
    "\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(pjunk_real_labels, pjunk_real_pred)))\n",
    "print(metrics.classification_report(pjunk_real_labels, pjunk_real_pred))\n",
    "print(metrics.confusion_matrix(pjunk_real_labels, pjunk_real_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting proton vs. Carbon + junk (w/ Simulated Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# proton 0s\n",
    "# Carbon+junk 1s\n",
    "pCjunk_sim = np.reshape(p_sim_charge + C_sim_charge + junk_sim_charge, (-1,1))\n",
    "pCjunk_sim_labels = np.hstack((p_sim_labels, C_sim_labels, junk_sim_labels))\n",
    "\n",
    "pCjunk_real = np.reshape(p_real_charge + C_real_charge + junk_real_charge, (-1,1))\n",
    "pCjunk_real_labels = np.hstack((p_real_labels, C_real_labels, junk_real_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pCjunk_sim_train, pCjunk_sim_test, pCjunk_sim_labels_train, pCjunk_sim_labels_test = train_test_split(pCjunk_sim, pCjunk_sim_labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = 0.1\n",
    "lr_pCjunk = LogisticRegression(C=reg)\n",
    "lr_pCjunk.fit(pCjunk_sim_train, pCjunk_sim_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6650111662944569\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00     10050\n",
      "        1.0       0.67      1.00      0.80     19951\n",
      "\n",
      "avg / total       0.44      0.67      0.53     30001\n",
      "\n",
      "[[    0 10050]\n",
      " [    0 19951]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#train on simulated + test on simulated \n",
    "pCjunk_sim_pred = lr_pCjunk.predict(pCjunk_sim_test)\n",
    "\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(pCjunk_sim_labels_test, pCjunk_sim_pred)))\n",
    "print(metrics.classification_report(pCjunk_sim_labels_test, pCjunk_sim_pred))\n",
    "print(metrics.confusion_matrix(pCjunk_sim_labels_test, pCjunk_sim_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7534399404983265\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       663\n",
      "        1.0       0.75      1.00      0.86      2026\n",
      "\n",
      "avg / total       0.57      0.75      0.65      2689\n",
      "\n",
      "[[   0  663]\n",
      " [   0 2026]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#train on simulated + test on real \n",
    "pCjunk_real_pred = lr_pCjunk.predict(pCjunk_real)\n",
    "\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(pCjunk_real_labels, pCjunk_real_pred)))\n",
    "print(metrics.classification_report(pCjunk_real_labels, pCjunk_real_pred))\n",
    "print(metrics.confusion_matrix(pCjunk_real_labels, pCjunk_real_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
