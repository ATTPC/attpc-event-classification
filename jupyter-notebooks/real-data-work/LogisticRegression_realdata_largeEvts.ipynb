{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading simulated data\n",
    "p_sim = sp.sparse.load_npz('../../data/tilt/20x20x20/pDisc_40000_20x20x20_tilt_largeEvts.npz')\n",
    "C_sim = sp.sparse.load_npz('../../data/tilt/20x20x20/CDisc_40000_20x20x20_tilt_largeEvts.npz')\n",
    "junk_sim = sp.sparse.load_npz('../../data/tilt/20x20x20/noiseDisc_40000_20x20x20.npz')\n",
    "\n",
    "#loading real data\n",
    "p_0130 = sp.sparse.load_npz('../../data/real/20x20x20/run_0130_pDisc.npz')\n",
    "C_0130 = sp.sparse.load_npz('../../data/real/20x20x20/run_0130_CDisc.npz')\n",
    "junk_0130 = sp.sparse.load_npz('../../data/real/20x20x20/run_0130_junkDisc.npz')\n",
    "p_0210 = sp.sparse.load_npz('../../data/real/20x20x20/run_0210_pDisc.npz')\n",
    "C_0210 = sp.sparse.load_npz('../../data/real/20x20x20/run_0210_CDisc.npz')\n",
    "junk_0210 = sp.sparse.load_npz('../../data/real/20x20x20/run_0210_junkDisc.npz')\n",
    "\n",
    "p_real = sp.sparse.vstack([p_0130, p_0210], format='csr')\n",
    "C_real = sp.sparse.vstack([C_0130, C_0210], format='csr')\n",
    "junk_real = sp.sparse.vstack([junk_0130, junk_0210], format='csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating labels\n",
    "p_sim_labels = np.zeros((p_sim.shape[0],))\n",
    "C_sim_labels = np.ones((C_sim.shape[0],))\n",
    "junk_sim_labels = np.ones((junk_sim.shape[0],))\n",
    "\n",
    "p_real_labels = np.zeros((p_real.shape[0],))\n",
    "C_real_labels = np.ones((C_real.shape[0],))\n",
    "junk_real_labels = np.ones((junk_real.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated proton events: 40001\n",
      "Simulated Carbon events: 40001\n",
      "Simulated junk events: 40000\n",
      "Real proton events: 663\n",
      "Real Carbon events: 340\n",
      "Real junk events: 1686\n"
     ]
    }
   ],
   "source": [
    "print(\"Simulated proton events: \" + str(p_sim.shape[0]))\n",
    "print(\"Simulated Carbon events: \" + str(C_sim.shape[0]))\n",
    "print(\"Simulated junk events: \" + str(junk_sim.shape[0]))\n",
    "\n",
    "print(\"Real proton events: \" + str(p_real.shape[0]))\n",
    "print(\"Real Carbon events: \" + str(C_real.shape[0]))\n",
    "print(\"Real junk events: \" + str(junk_real.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting proton vs. Carbon (w/ Simulated Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pC_sim = sp.sparse.vstack([p_sim, C_sim], format='csr')\n",
    "pC_sim_labels = np.hstack((p_sim_labels, C_sim_labels))\n",
    "\n",
    "pC_real = sp.sparse.vstack([p_real, C_real], format='csr')\n",
    "pC_real_labels = np.hstack((p_real_labels, C_real_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pC_sim_train, pC_sim_test, pC_sim_labels_train, pC_sim_labels_test = train_test_split(pC_sim, pC_sim_labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = 0.001\n",
    "lr_pC = LogisticRegression(C=reg)\n",
    "lr_pC.fit(pC_sim_train, pC_sim_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8865056747162642\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.88      0.89      0.89     10007\n",
      "        1.0       0.89      0.88      0.89      9994\n",
      "\n",
      "avg / total       0.89      0.89      0.89     20001\n",
      "\n",
      "[[8939 1068]\n",
      " [1202 8792]]\n"
     ]
    }
   ],
   "source": [
    "#train on simulated + test on simulated \n",
    "pC_sim_pred = lr_pC.predict(pC_sim_test)\n",
    "\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(pC_sim_labels_test, pC_sim_pred)))\n",
    "print(metrics.classification_report(pC_sim_labels_test, pC_sim_pred))\n",
    "print(metrics.confusion_matrix(pC_sim_labels_test, pC_sim_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.43668993020937186\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.94      0.16      0.27       663\n",
      "        1.0       0.37      0.98      0.54       340\n",
      "\n",
      "avg / total       0.75      0.44      0.36      1003\n",
      "\n",
      "[[105 558]\n",
      " [  7 333]]\n"
     ]
    }
   ],
   "source": [
    "#train on simulated + test on real\n",
    "pC_real_pred = lr_pC.predict(pC_real)\n",
    "\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(pC_real_labels, pC_real_pred)))\n",
    "print(metrics.classification_report(pC_real_labels, pC_real_pred))\n",
    "print(metrics.confusion_matrix(pC_real_labels, pC_real_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulated Data  C=1e-05 accuracy=0.7718614069296535\n",
      "Real Data       C=1e-05 accuracy=0.3469591226321037\n",
      "Simulated Data  C=0.0001 accuracy=0.8599570021498925\n",
      "Real Data       C=0.0001 accuracy=0.3708873379860419\n",
      "Simulated Data  C=0.001 accuracy=0.8865056747162642\n",
      "Real Data       C=0.001 accuracy=0.43668993020937186\n",
      "Simulated Data  C=0.01 accuracy=0.8867056647167642\n",
      "Real Data       C=0.01 accuracy=0.5024925224327019\n",
      "Simulated Data  C=0.1 accuracy=0.8769561521923904\n",
      "Real Data       C=0.1 accuracy=0.5244267198404786\n",
      "Simulated Data  C=1.0 accuracy=0.8736063196840158\n",
      "Real Data       C=1.0 accuracy=0.5274177467597209\n",
      "Simulated Data  C=10.0 accuracy=0.8728563571821409\n",
      "Real Data       C=10.0 accuracy=0.5274177467597209\n",
      "Simulated Data  C=100.0 accuracy=0.8728063596820159\n",
      "Real Data       C=100.0 accuracy=0.5274177467597209\n",
      "Simulated Data  C=1000.0 accuracy=0.8728063596820159\n",
      "Real Data       C=1000.0 accuracy=0.5274177467597209\n",
      "Simulated Data  C=10000.0 accuracy=0.8728063596820159\n",
      "Real Data       C=10000.0 accuracy=0.5274177467597209\n"
     ]
    }
   ],
   "source": [
    "C_vals = [10e-6, 10e-5, 10e-4, 10e-3, 10e-2, 10e-1, 10e0, 10e1, 10e2, 10e3]\n",
    "\n",
    "precisions_sim = []\n",
    "recalls_sim = []\n",
    "f1s_sim = []\n",
    "accuracies_sim = []\n",
    "\n",
    "precisions_real = []\n",
    "recalls_real = []\n",
    "f1s_real = []\n",
    "accuracies_real = []\n",
    "\n",
    "for c in C_vals:\n",
    "    pC_sim_pred = LogisticRegression(C=c).fit(pC_sim_train, pC_sim_labels_train).predict(pC_sim_test)\n",
    "    \n",
    "    precisions_sim.append(metrics.precision_score(pC_sim_labels_test, pC_sim_pred))\n",
    "    recalls_sim.append(metrics.recall_score(pC_sim_labels_test, pC_sim_pred))\n",
    "    f1s_sim.append(metrics.f1_score(pC_sim_labels_test, pC_sim_pred))\n",
    "    accuracies_sim.append(metrics.accuracy_score(pC_sim_labels_test, pC_sim_pred))\n",
    "    print(\"Simulated Data  C=\" + str(c) + \" accuracy=\" + str(metrics.accuracy_score(pC_sim_labels_test, pC_sim_pred)))\n",
    "    \n",
    "    pC_real_pred = LogisticRegression(C=c).fit(pC_sim_train, pC_sim_labels_train).predict(pC_real)\n",
    "    \n",
    "    precisions_real.append(metrics.precision_score(pC_real_labels, pC_real_pred))\n",
    "    recalls_real.append(metrics.recall_score(pC_real_labels, pC_real_pred))\n",
    "    f1s_real.append(metrics.f1_score(pC_real_labels, pC_real_pred))\n",
    "    accuracies_real.append(metrics.accuracy_score(pC_real_labels, pC_real_pred))\n",
    "    print(\"Real Data       C=\" + str(c) + \" accuracy=\" + str(metrics.accuracy_score(pC_real_labels, pC_real_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(len(C_vals))\n",
    "\n",
    "plt.plot(C_vals, accuracies_sim)\n",
    "plt.plot(C_vals, accuracies_real)\n",
    "\n",
    "plt.xscale('log')\n",
    "        \n",
    "plt.xlabel('C (inverse regularization constant)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy by Regularization')\n",
    "plt.legend(['Simulated Data', 'Real Data'], loc='lower right')\n",
    "\n",
    "#plt.savefig('../../plots/results/real/LR_pC_accuracyxC.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(len(C_vals))\n",
    "\n",
    "plt.plot(C_vals, f1s_sim)\n",
    "plt.plot(C_vals, f1s_real)\n",
    "\n",
    "plt.xscale('log')\n",
    "        \n",
    "plt.xlabel('C (inverse regularization constant)')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score by Regularization')\n",
    "plt.legend(['Simulated Data', 'Real Data'], loc='lower right')\n",
    "\n",
    "#plt.savefig('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting proton vs. Carbon + junk (w/ Simulated Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# proton 0s\n",
    "# Carbon+junk 1s\n",
    "pCjunk_sim = sp.sparse.vstack([p_sim, C_sim, junk_sim], format='csr')\n",
    "pCjunk_sim_labels = np.hstack((p_sim_labels, C_sim_labels, junk_sim_labels))\n",
    "\n",
    "pCjunk_real = sp.sparse.vstack([p_real, C_real, junk_real], format='csr')\n",
    "pCjunk_real_labels = np.hstack((p_real_labels, C_real_labels, junk_real_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pCjunk_sim_train, pCjunk_sim_test, pCjunk_sim_labels_train, pCjunk_sim_labels_test = train_test_split(pCjunk_sim, pCjunk_sim_labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 0.001\n",
    "lr_pCjunk = LogisticRegression(C=reg)\n",
    "lr_pCjunk.fit(pCjunk_sim_train, pCjunk_sim_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on simulated + test on simulated \n",
    "pCjunk_sim_pred = lr_pCjunk.predict(pCjunk_sim_test)\n",
    "\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(pCjunk_sim_labels_test, pCjunk_sim_pred)))\n",
    "print(metrics.classification_report(pCjunk_sim_labels_test, pCjunk_sim_pred))\n",
    "print(metrics.confusion_matrix(pCjunk_sim_labels_test, pCjunk_sim_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on simulated + test on real \n",
    "pCjunk_real_pred = lr_pCjunk.predict(pCjunk_real)\n",
    "\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(pCjunk_real_labels, pCjunk_real_pred)))\n",
    "print(metrics.classification_report(pCjunk_real_labels, pCjunk_real_pred))\n",
    "print(metrics.confusion_matrix(pCjunk_real_labels, pCjunk_real_pred))\n",
    "\n",
    "print(pCjunk_real_pred.shape)\n",
    "print(np.sum(pCjunk_real_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting proton vs. Carbon vs. junk (w/ Simulated Noise) - **multi-class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#protons 0\n",
    "#carbons 1\n",
    "#junk 2\n",
    "\n",
    "#create junk 2s labels\n",
    "junkMC_sim_labels = np.full(junk_sim_labels.shape, 2)\n",
    "junkMC_real_labels = np.full(junk_real_labels.shape, 2)\n",
    "\n",
    "multi_sim = sp.sparse.vstack([p_sim, C_sim, junk_sim], format='csr')\n",
    "multi_sim_labels = np.hstack((p_sim_labels, C_sim_labels, junkMC_sim_labels))\n",
    "\n",
    "multi_real = sp.sparse.vstack([p_real, C_real, junk_real], format='csr')\n",
    "multi_real_labels = np.hstack((p_real_labels, C_real_labels, junkMC_real_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multi_sim_labels.shape)\n",
    "print(multi_real_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multi_sim_train, multi_sim_test, multi_sim_labels_train, multi_sim_labels_test = train_test_split(multi_sim, multi_sim_labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 0.001\n",
    "lr_multi = LogisticRegression(C=reg, multi_class='ovr')\n",
    "lr_multi.fit(multi_sim_train, multi_sim_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on simulated + test on simulated \n",
    "multi_sim_pred = lr_multi.predict(multi_sim_test)\n",
    "\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(multi_sim_labels_test, multi_sim_pred)))\n",
    "print(metrics.classification_report(multi_sim_labels_test, multi_sim_pred))\n",
    "print(metrics.confusion_matrix(multi_sim_labels_test, multi_sim_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on simulated + test on simulated \n",
    "multi_real_pred = lr_multi.predict(multi_real)\n",
    "\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(multi_real_labels, multi_real_pred)))\n",
    "print(metrics.classification_report(multi_real_labels, multi_real_pred))\n",
    "print(metrics.confusion_matrix(multi_real_labels, multi_real_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predicting proton vs. junk (w/ Simulated Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# proton 0s\n",
    "# junk 1s\n",
    "pjunk_sim = sp.sparse.vstack([p_sim, junk_sim], format='csr')\n",
    "pjunk_sim_labels = np.hstack((p_sim_labels, junk_sim_labels))\n",
    "\n",
    "pjunk_real = sp.sparse.vstack([p_real, junk_real], format='csr')\n",
    "pjunk_real_labels = np.hstack((p_real_labels, junk_real_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pjunk_sim_train, pjunk_sim_test, pjunk_sim_labels_train, pjunk_sim_labels_test = train_test_split(pjunk_sim, pjunk_sim_labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 0.001\n",
    "lr_pjunk = LogisticRegression(C=reg)\n",
    "lr_pjunk.fit(pjunk_sim_train, pjunk_sim_labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on simulated + test on simulated \n",
    "pjunk_sim_pred = lr_pjunk.predict(pjunk_sim_test)\n",
    "\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(pjunk_sim_labels_test, pjunk_sim_pred)))\n",
    "print(metrics.classification_report(pjunk_sim_labels_test, pjunk_sim_pred))\n",
    "print(metrics.confusion_matrix(pjunk_sim_labels_test, pjunk_sim_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train on simulated + test on real \n",
    "pjunk_real_pred = lr_pjunk.predict(pjunk_real)\n",
    "\n",
    "print(\"Accuracy: \" + str(metrics.accuracy_score(pjunk_real_labels, pjunk_real_pred)))\n",
    "print(metrics.classification_report(pjunk_real_labels, pjunk_real_pred))\n",
    "print(metrics.confusion_matrix(pjunk_real_labels, pjunk_real_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
